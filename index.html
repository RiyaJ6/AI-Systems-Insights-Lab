// server.js - Node/Express proxy for Live mode (Railway)
const express = require('express')
const fetch = require('node-fetch') // railway's node image supports node 18+; otherwise install node-fetch
const bodyParser = require('body-parser')

const app = express()
app.use(bodyParser.json({limit:'5mb'}))

const HF_MODEL = process.env.HF_MODEL || 'google/flan-t5-small' // light model
const HF_TOKEN = process.env.HF_TOKEN // set as Railway secret

app.get('/', (req,res)=> res.send('AI Insights Proxy alive'))

app.post('/analyze', async (req,res) => {
  try{
    if(!HF_TOKEN) return res.status(500).json({error:'HF_TOKEN not configured'})

    const { documents = [], prompt = 'Summarize' } = req.body
    const text = documents.map(d => `FILE: ${d.name}\\n${d.text}`).join('\\n\\n')
    const modelInput = `${prompt}\\n\\n${text}`.slice(0, 20000) // avoid very long input

    // call Hugging Face Inference API
    const hfResp = await fetch(`https://api-inference.huggingface.co/models/${HF_MODEL}`, {
      method: 'POST',
      headers: {
        'Authorization': `Bearer ${HF_TOKEN}`,
        'Content-Type': 'application/json'
      },
      body: JSON.stringify({ inputs: modelInput, options:{wait_for_model:true, use_cache:false}, parameters:{max_new_tokens:300} })
    })

    if(!hfResp.ok){
      const txt = await hfResp.text().catch(()=>'<no-body>')
      return res.status(502).json({error:'HF inference failed', details: txt})
    }

    const hfJson = await hfResp.json()
    let generated = ''
    if(Array.isArray(hfJson) && hfJson[0]?.generated_text) generated = hfJson[0].generated_text
    else if(hfJson.generated_text) generated = hfJson.generated_text
    else generated = JSON.stringify(hfJson).slice(0,1000)

    // Simple keywords (top repeated words)
    const combined = documents.map(d=>d.text).join(' ')
    const words = combined.toLowerCase().replace(/[^a-z0-9\\s]/g,' ').split(/\\s+/).filter(w=>w.length>3)
    const freq = {}
    words.forEach(w => freq[w] = (freq[w]||0) + 1)
    const top = Object.entries(freq).sort((a,b)=>b[1]-a[1]).slice(0,8)
    const labels = top.map(t=>t[0])
    const data = top.map(t=>t[1])

    // small scatter mock (randomized)
    const scatterPoints = documents.map((d,i)=>[i, (freq[labels[0]]||1) * (Math.random()*0.5 + 1)])

    const insights = [
      {title:'Summary (model)', text: generated.slice(0,1200)},
      {title:'Files processed', text: documents.map(d=>d.name).join(', ')},
      {title:'Prompt', text: prompt}
    ]

    res.json({insights, chart:{labels,data}, scatterPoints})
  }catch(e){
    console.error(e)
    res.status(500).json({error: e.message})
  }
})

const PORT = process.env.PORT || 8080
app.listen(PORT, ()=> console.log('Server listening on', PORT))
